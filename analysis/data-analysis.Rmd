---
title: "Dehumanization Project Data Analysis"
output: html_notebook
---

The R Version used to create this file is: `R version 4.3.2 (2023-10-31)`. 
To reproduce analysis, switch with your own version of R by pressing `Tools` > `Global Options` > `R Version`

Goal of the logistical regression: We would like to determine whether the agents expressing dehumanizing 
reactions are selected more frequently than those expressing non-dehumanizing reactions.


### Installing and downloading packages and preparing for analysis
```{r Download packages and format csv to prepare for analysis}

knitr::opts_knit$set(root.dir = '/Users/elizabethmiclau/Downloads/Dehumanization Project')

# Change out the working directory for your own

install.packages("remotes")
if(!suppressWarnings(require(groundhog))){remotes::install_github('CredibilityLab/groundhog')}
library("groundhog")

# Put all packages in and call groundhog
pkgs <- c(
    "aws.s3",
    "tidyverse"
    )

pkgs_all <- c(pkgs)

# Change out the date for today's date (unless you are running an old analysis)
groundhog.library(pkgs_all, "2024-1-7")


# Importing the data: 
full_data_widelong <- read.csv("~/Downloads/Dehumanization Project/full_data_widelong.csv")

# Necessary libraries
library(dplyr)
library(ggplot2)

# Exclude the last row with prompt descriptions for these descriptive analyses:
full_data_widelong <- full_data_widelong[-nrow(full_data_widelong), ]

```


### Participant Demographics - 
```{r Number of Participants}

# Number of Participants
unique_participants <- full_data_widelong %>% distinct(Prolific_ID, .keep_all = TRUE) # Filtering unique participants based on Prolific_ID
num_participants <- nrow(unique_participants) # Count unique participants
print(paste("Number of Participants:", num_participants))

```
 
```{r Gender Distribution of Participants}

# Gender Distribution
gender_stats <- table(unique_participants$Gender)
print(gender_stats)

```

```{r Age Distribution of Participants}

age_stats <- summary(unique_participants$Age)
print(age_stats)
ggplot(unique_participants, aes(x=Age)) + 
  geom_histogram(binwidth=1, fill="blue", color="black") +
  scale_x_continuous(breaks=seq(10, 80, by=10), limits=c(10, 80)) +
  ylim(0, 5) +
  labs(title="Age Distribution of Unique Participants", x="Age", y="Count") +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))
```

```{r Race Distribution of Participants}

# Race Distribution
race_stats <- table(unique_participants$Race)
print(race_stats)
ggplot(unique_participants, aes(x=Race, fill=Race)) +
  geom_bar() +
  labs(title="Race Distribution of Participants", x="Race", y="Count") +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))

```

```{r Income Distribution of Participants}

# Income Distribution
income_ordered <- c("Less than $40,000", "$40,000-$79,999", "$80,000-$119,999", "$120,000-$199,999", "More than $200,000")
unique_participants$Income <- factor(unique_participants$Income, levels = income_ordered)
income_stats <- table(unique_participants$Income)
print(income_stats)
ggplot(unique_participants, aes(x=Income, fill=Income)) +
  geom_bar() +
  labs(title="Income Distribution of Participants", x="Income", y="Count") +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))

```

```{r Conservatism Distribution of Participants}

# Conservatism Distribution
conservatism_ordered <- c("Extremely Liberal", "Liberal", "Somewhat Liberal", "Middle of the Road", "Somewhat Conservative", "Conservative", "Extremely Conservative")
unique_participants$Conservatism <- factor(unique_participants$Conservatism, levels = conservatism_ordered)
conservatism_stats <- table(unique_participants$Conservatism)
print(conservatism_stats)
ggplot(unique_participants, aes(x=Conservatism, fill=Conservatism)) +
  geom_bar() +
  labs(title="Conservatism Distribution of Participants", x="Conservatism", y="Count") +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))

```

```{r Education Distribution of Participants}

# Education Distribution
education_ordered <- c("High school/GED", "Some college experience", "2-year college degree (A.A.)", "4-year college degree (BA/BS)", "Graduate or Professional Degree (MA/ MBA/ MS/ PhD)")
unique_participants$Education <- factor(unique_participants$Education, levels = education_ordered)
education_stats <- table(unique_participants$Education)
print(education_stats)
ggplot(unique_participants, aes(x=Education, fill=Education)) +
  geom_bar() +
  labs(title="Education Distribution of Participants", x="Education", y="Count") +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))

```


### Running a Binomial Test

#### In our study, we aim to determine whether participants exhibit a preference for selecting agents that express a dehumanizing reaction (DH) over those that express a non-dehumanizing reaction (NDH), beyond what would be expected by mere chance. To evaluate this, we have chosen to conduct a Binomial Test.

```{r Binomial Test }

# Loading the dataset
full_data_widelong <- read.csv("~/Downloads/Dehumanization Project/full_data_widelong.csv")

# Check unique values in 'choice' column to ensure it's binary
full_data_widelong$choice <- factor(full_data_widelong$choice)
unique(full_data_widelong$choice)

# Count the number of DH choices
num_DH_choices <- sum(as.character(full_data_widelong$choice) == "DH", na.rm = TRUE)
num_DH_choices <- as.integer(num_DH_choices)
total_choices <- sum(!is.na(full_data_widelong$choice))

# Perform the Binomial Test
binomial_test <- binom.test(x = num_DH_choices, n = total_choices, p = 0.5)
print(binomial_test)

# Interpreting the results of the Binomial Test:
# The p-value of < 2.2e-16 indicates that the choice of DH is significantly different from chance.

```


### Running a Mixed-effects Model

#### This new approach provides insights into the variability of choices between "DH" and "NDH" across participants, accounting for the fact that multiple choices come from the same participant.

```{r Nested Data - Mixed-effects Model}

install.packages("lme4")
install.packages("lmerTest")
library(lme4)
library(lmerTest) # For summaries including p-values

# Loading the dataset
full_data_long_test <- read.csv("~/Downloads/Dehumanization Project/full_data_long.csv")


# Preprocessing the data
full_data_long_test <- subset(full_data_long_test, !is.na(trial)) # Remove phone and gender rows
full_data_long_test <- subset(full_data_long_test, choice != 0) # Remove rows where 'choice' is 0
full_data_long_test$choice <- ifelse(full_data_long_test$rating == "DH", 1, 0) # Convert 'choice' from DH/NDH to 1/0

# Fit a mixed-effects logistic regression model. The (1 | Prolific_ID) adds a random effect for each participant.
model <- glmer(choice ~ 1 + (1 | Prolific_ID), family = binomial, data = full_data_long_test)

# View the summary of the model
summary(model)


# 1. Visual Summary: Individual differences in the choice patterns
library(ggplot2) # Load necessary library
full_data_long_test$choice <- as.numeric(full_data_long_test$rating == "DH") # Counting DH choices
proportions <- aggregate(choice ~ Prolific_ID, data = full_data_long_test, mean) # Proportion of DH choices
ggplot(proportions, aes(x = Prolific_ID, y = choice)) + # Plotting
  geom_bar(stat = "identity") +
  theme(axis.text.x = element_text(angle = 90, hjust = 1)) +
  labs(title = "Proportion of DH Choices per Participant", x = "Participant ID", y = "Proportion of DH Choices")

# Interpretation of the Output:
  # AIC/BIC (Akaike and Bayesian Information Criterion): These values are useful for comparing different models fit to this data to see if they provide a better balance of goodness of fit and simplicity. If this is the only model you are considering, then the AIC and BIC give you a baseline for comparison if you decide to add more predictors or make other adjustments to your model.
  # Random Effects: Tell us much variability there is in the choices among different participants. For us, the random effect of Prolific_ID shows some level of variability across participants.
  # Fixed Effects: The fixed effect is the intercept; Tells us the log odds of choosing DH (versus NDH) when all other variables are held at zero.
  # The z-value and p-value (Pr(>|z|)) for the intercept test whether it is significantly different from zero. A significant intercept in a model with no other predictors suggests that there is an overall tendency towards one choice over the other in the population. So, since ours is negative and is statistically significant, this suggests that the overall tendency is towards choosing NDH over DH.


# 2. Visualize Random Effects: How participant-level intercepts vary around the overall intercept
library(lattice) # Load necessary library
ranef_plot <- dotplot(ranef(model, condVar = TRUE)) # Extract and plot random effects
print(ranef_plot)

# Interpretation of the Plot:
  # Fair amount of variability among participants, with some showing a much higher or lower tendency to choose DH compared to the average (the vertical line at x=0). The intercept for some participants is significantly different from zero, which reinforces the appropriateness of using a mixed-effects model for your data since it accounts for this individual variability.


# 3. Visualize the residuals of our mixed-effects model
plot(residuals(model), main="Residuals Plot")

# Interpretation of the Plot:
  # Clusters at two levels is expected in logistic regression. For many observations, the model's predictions are close to the actual values.
 
 # No obvious pattern to the residuals, suggesting the model's variance is constant across different levels of prediction (homoscedasticity).
  # A few residuals with higher absolute values. Possibly outliars or the model didn't fit very well?


# 4. Visualize the predicted probabilities of choosing DH for each participant.
full_data_long_test$predicted_prob <- predict(model, type = "response")
ggplot(full_data_long_test, aes(x = Prolific_ID, y = predicted_prob)) +
  geom_jitter(width = 0.2, alpha = 0.5) +
  theme(axis.text.x = element_text(angle = 90, hjust = 1)) +
  labs(title = "Predicted Probabilities of DH Choice per Participant", 
       x = "Participant ID", y = "Predicted Probability")

# Interpretation of the Plot:
  # The y-axis shows the predicted probability that the participant will choose DH (0 is no chance, 1 is certainty)
  # The vertical spread of points indicates there is some variation among participants in their probability of choosing DH (i.e.) participants have different propensities for choosing DH. Our mixed-effects model accounts for the individual-level variation!
  # Most predicted probabilities are below 0.5, indicating a general tendency away from choosing DH among participants.


```

### Running Pearson correlations

### We are interested in looking at the zero-order correlations between the "proportions" variable (indicating the proportion of DH responses selected out of the ten total trials) and the following: Current general affect, Current discrete emotions, Individual differences in support for moral exclusion, Political orientation of participants, Income, Age, and Gender.

```{r Zero-order / Pearson correlations}

# Merge proportions table with the full_data_long_test dataset
install.packages("dplyr") # Installing required package
library(dplyr)
proportions <- proportions %>% # Renaming column in proportions
  rename(proportion = choice) 
full_data_long_test <- merge(full_data_long_test, proportions[, c("Prolific_ID", "proportion")], by = "Prolific_ID", all.x = TRUE)

# Remove 'gender' and 'device' columns
full_data_long_test <- subset(full_data_long_test, select = -c(gender, device))



#  -----------------------------------------------------------------------------------------
# 1. Current general affect 

# Assumptions of Pearson's Correlation: 

## a) Both variables are metric
class(full_data_long_test$proportion) # Proportion is numeric. 
class(full_data_long_test$Q2_1) # Affect is an integer.

## b) Linear relationship between the two variables:
library(ggplot2)
ggplot(full_data_long_test, aes(x = Q2_1, y = proportion)) +
    geom_point() +
    geom_smooth(method = "lm", color = "blue") +
    ggtitle("Scatter plot of Proportion by Q2_1")
## Do not have a clear linear relationship!!

## c) Normal distribution:
library(ggplot2)
hist(full_data_long_test$proportion, xlab = "proportion", main = "Histogram of proportion")
hist(full_data_long_test$Q2_1, xlab = "Current General Affect", main = "Histogram of Current General Affect")
## Also does not have a normal distribution!!
        
## Does not pass the assumption checks >> Not valid to calculate a Pearson's Correlation Coefficient.

## Hypothetically, this would yield a correlation value of 0.08936236
cor_test_Q2_1 <- cor.test(full_data_long_test$proportion, full_data_long_test$Q2_1, method = "pearson")
print(cor_test_Q2_1)


#  -----------------------------------------------------------------------------------------
# 2. Current discrete emotions

# Calculating Cronbach's alpha of variables for 'discrete emotions' (Q3):
install.packages("psych") # Installing required package
library(psych)
full_data_long_test$Q3_1 <- as.numeric(as.character(full_data_long_test$Q3_1)) # Converting chr. to numeric
full_data_long_test$Q3_2 <- as.numeric(as.character(full_data_long_test$Q3_2))
full_data_long_test$Q3_3 <- as.numeric(as.character(full_data_long_test$Q3_3))
full_data_long_test$Q3_4 <- as.numeric(as.character(full_data_long_test$Q3_4))
full_data_long_test$Q3_5 <- as.numeric(as.character(full_data_long_test$Q3_5))
full_data_long_test$Q3_6 <- as.numeric(as.character(full_data_long_test$Q3_6))
q3items <- full_data_long_test[, c("Q3_1", "Q3_2", "Q3_3", "Q3_4", "Q3_5", "Q3_6")]
summary(q3items)

alpha_result <- psych::alpha(q3items)
str(alpha_result)
# Cronbach's Alpha (raw_alpha): 0.937 <- Well above the 0.8 threshold !

# Create an average of the Q3 items: new column named "Q3_avg"
full_data_long_test$Q3_avg <- rowMeans(full_data_long_test[, c("Q3_1", "Q3_2", "Q3_3", "Q3_4", "Q3_5", "Q3_6")], na.rm = TRUE)

# Assumptions of Pearson's Correlation: 

## a) Both variables are metric
class(full_data_long_test$proportion) # Proportion is numeric. 
class(full_data_long_test$Q3_avg) # Q3_Avg is numeric.

## b) Linear relationship between the two variables:
library(ggplot2)
ggplot(full_data_long_test, aes(x = Q3_avg, y = proportion)) +
    geom_point() +
    geom_smooth(method = "lm", color = "blue") +
    ggtitle("Scatter plot of Proportion by Current Discrete Emotions")
## Do not have a clear linear relationship!!

## c) Normal distribution:
library(ggplot2)
hist(full_data_long_test$proportion, xlab = "proportion", main = "Histogram of proportion")
hist(full_data_long_test$Q3_avg, xlab = "Current Discrete Emotions", main = "Histogram of Current Discrete Emotions")
## Variables do not have normal distributions!!
        
## Does not pass the assumption checks >> Not valid to calculate a Pearson's Correlation Coefficient.

## Hypothetically, this would yield a correlation value of 0.003770348
cor_test_Q3_avg <- cor.test(full_data_long_test$proportion, full_data_long_test$Q3_avg, method = "pearson")
print(cor_test_Q3_avg)


#  -----------------------------------------------------------------------------------------
# 3. Individual differences in support for moral exclusion


# Calculating Cronbach's alpha of variables for 'moral exclusion' (Q5):
install.packages("psych") # Installing required package
library(psych)
full_data_long_test$Q5_1 <- as.numeric(as.character(full_data_long_test$Q5_1)) # Converting chr. to numeric
full_data_long_test$Q5_2 <- as.numeric(as.character(full_data_long_test$Q5_2))
full_data_long_test$Q5_3 <- as.numeric(as.character(full_data_long_test$Q5_3))
full_data_long_test$Q5_4 <- as.numeric(as.character(full_data_long_test$Q5_4))
full_data_long_test$Q5_5 <- as.numeric(as.character(full_data_long_test$Q5_5))
full_data_long_test$Q5_6 <- as.numeric(as.character(full_data_long_test$Q5_6))
full_data_long_test$Q5_7 <- as.numeric(as.character(full_data_long_test$Q5_7))
q5items <- full_data_long_test[, c("Q5_1", "Q5_2", "Q5_3", "Q5_4", "Q5_5", "Q5_6", "Q5_7")]
summary(q5items)

alpha_result <- psych::alpha(q5items)
str(alpha_result)
# Cronbach's Alpha (raw_alpha): 0.955 <- Well above the 0.8 threshold !

# Create an average of the Q5 items: new column named "Q5_avg"
full_data_long_test$Q5_avg <- rowMeans(full_data_long_test[, c("Q5_1", "Q5_2", "Q5_3", "Q5_4", "Q5_5", "Q5_6", "Q5_7")], na.rm = TRUE)

# Assumptions of Pearson's Correlation: 

## a) Both variables are metric
class(full_data_long_test$proportion) # Proportion is numeric. 
class(full_data_long_test$Q5_avg) # Q5_Avg is numeric.

## b) Linear relationship between the two variables:
library(ggplot2)
ggplot(full_data_long_test, aes(x = Q5_avg, y = proportion)) +
    geom_point() +
    geom_smooth(method = "lm", color = "blue") +
    ggtitle("Scatter plot of Proportion by Support for Moral Exclusion")
## Loosely linear relationship!!

## c) Normal distribution:
library(ggplot2)
hist(full_data_long_test$proportion, xlab = "proportion", main = "Histogram of proportion")
hist(full_data_long_test$Q5_avg, xlab = "Current Moral Exclusion", main = "Histogram of Individual Differences in Support for Moral Exclusion")
## Variables do not have normal distributions!!
        
## Does not pass the assumption checks >> Not valid to calculate a Pearson's Correlation Coefficient.

## Hypothetically, this would yield a correlation value of 0.2361316
cor_test_Q5_avg <- cor.test(full_data_long_test$proportion, full_data_long_test$Q5_avg, method = "pearson")
print(cor_test_Q5_avg)



#  -----------------------------------------------------------------------------------------
# 4. Political orientation of participants

# Converting the 'Conservatism' variable into a numeric value, using the following scale:
#1. Extremely Liberal
#2. Liberal
#3. Somewhat Liberal
#4. Middle of the Road
#5. Somewhat Conservative
#6. Conservative
#7. Extremely Conservative

# Create Conservatism.no column with corresponding numerical values
levels <- c("Extremely Liberal", "Liberal", "Somewhat Liberal",
            "Middle of the Road", "Somewhat Conservative", 
            "Conservative", "Extremely Conservative")
numbers <- 1:7
full_data_long_test$Conservatism.no <- match(full_data_long_test$Conservatism, levels) 


# Assumptions of Pearson's Correlation: 

## a) Both variables are metric
class(full_data_long_test$proportion) # Proportion is numeric. 
class(full_data_long_test$Conservatism.no) # Conservatism is an integer.

## b) Linear relationship between the two variables:
library(ggplot2)
ggplot(full_data_long_test, aes(x = Conservatism.no, y = proportion)) +
    geom_point() +
    geom_smooth(method = "lm", color = "blue") +
    ggtitle("Scatter plot of Proportion by Conservatism")
## Do not have a clear linear relationship!!

## c) Normal distribution:
library(ggplot2)
hist(full_data_long_test$proportion, xlab = "proportion", main = "Histogram of proportion")
hist(full_data_long_test$Conservatism.no, xlab = "Conservatism", main = "Histogram of Conservatism")
## Also does not have a normal distribution!!
        
## Does not pass the assumption checks >> Not valid to calculate a Pearson's Correlation Coefficient.

## Hypothetically, this would yield a correlation value of 0.3901844
cor_test_Conservatism <- cor.test(full_data_long_test$proportion, full_data_long_test$Conservatism.no, method = "pearson")
print(cor_test_Conservatism)
## A value of 0.39 would indicate a Low or Moderate Correlation

  
  #  -----------------------------------------------------------------------------------------
# 4. Income 

# Converting the 'Income' variable into a numeric value, using the following scale:
#1. Less than $40,000
#2. $40,000-$79,999
#3. $80,000-$119,999
#4. $120,000-$199,999
#5. More than $200,000

# Create Conservatism.no column with corresponding numerical values
levels <- c("Less than $40,000", "$40,000-$79,999", "$80,000-$119,999",
            "$120,000-$199,999", "More than $200,000")
numbers <- 1:5
full_data_long_test$Income.no <- match(full_data_long_test$Income, levels) 


# Assumptions of Pearson's Correlation: 

## a) Both variables are metric
class(full_data_long_test$proportion) # Proportion is numeric. 
class(full_data_long_test$Income.no) # Income is an integer.

## b) Linear relationship between the two variables:
library(ggplot2)
ggplot(full_data_long_test, aes(x = Income.no, y = proportion)) +
    geom_point() +
    geom_smooth(method = "lm", color = "blue") +
    ggtitle("Scatter plot of Proportion by Income")
## Do not have a clear linear relationship!!

## c) Normal distribution:
library(ggplot2)
hist(full_data_long_test$proportion, xlab = "proportion", main = "Histogram of proportion")
hist(full_data_long_test$Income.no, xlab = "Income", main = "Histogram of Income")
## Also does not have a normal distribution!!
        
## Does not pass the assumption checks >> Not valid to calculate a Pearson's Correlation Coefficient.

## Hypothetically, this would yield a correlation value of 0.07293129 
cor_test_Income <- cor.test(full_data_long_test$proportion, full_data_long_test$Income.no, method = "pearson")
print(cor_test_Income)



  #  -----------------------------------------------------------------------------------------
# 5. Age 

# Assumptions of Pearson's Correlation: 

## a) Both variables are metric
class(full_data_long_test$proportion) # Proportion is numeric. 
class(full_data_long_test$Age) # Age is an integer.

## b) Linear relationship between the two variables:
library(ggplot2)
ggplot(full_data_long_test, aes(x = Age, y = proportion)) +
    geom_point() +
    geom_smooth(method = "lm", color = "blue") +
    ggtitle("Scatter plot of Proportion by Age")
## Do not have a clear linear relationship!!

## c) Normal distribution:
library(ggplot2)
hist(full_data_long_test$proportion, xlab = "proportion", main = "Histogram of proportion")
hist(full_data_long_test$Income.no, xlab = "Age", main = "Histogram of Age")
## Also does not have a normal distribution!!
        
## Does not pass the assumption checks >> Not valid to calculate a Pearson's Correlation Coefficient.

## Hypothetically, this would yield a correlation value of 0.121015  
cor_test_Age <- cor.test(full_data_long_test$proportion, full_data_long_test$Age, method = "pearson")
print(cor_test_Age)




  #  -----------------------------------------------------------------------------------------
# 6. Gender 

# Converting the 'Gender' variable into a binary value, using the following scale:
#1. Female
#2. Male
#3. Other

# Create Gender.no column with corresponding numerical values
levels <- c("Female", "Male", "Other")
numbers <- 1:3
full_data_long_test$Gender.no <- match(full_data_long_test$Gender, levels) 


# Assumptions of Pearson's Correlation: 

## a) Both variables are metric
class(full_data_long_test$proportion) # Proportion is numeric. 
class(full_data_long_test$Gender.no) # Gender is an integer.

## b) Linear relationship between the two variables:
library(ggplot2)
ggplot(full_data_long_test, aes(x = Gender.no, y = proportion)) +
    geom_point() +
    geom_smooth(method = "lm", color = "blue") +
    ggtitle("Scatter plot of Proportion by Gender")
## Do not have a clear linear relationship!!

## c) Normal distribution:
library(ggplot2)
hist(full_data_long_test$proportion, xlab = "proportion", main = "Histogram of proportion")
hist(full_data_long_test$Gender.no, xlab = "Gender", main = "Histogram of Gender")
## Also does not have a normal distribution!!
        
## Does not pass the assumption checks >> Not valid to calculate a Pearson's Correlation Coefficient.

## Hypothetically, this would yield a correlation value of 0.03082222 
cor_test_Gender <- cor.test(full_data_long_test$proportion, full_data_long_test$Gender.no, method = "pearson")
print(cor_test_Gender)


# Saving this updated data frame, just in case.
write.csv(full_data_long_test, "full_data_long_test.csv", row.names = FALSE)


```

### Reaction times in choosing DH versus NDH statements -- Mixed Model

### We are interested in comparing the reaction times in choosing dehumanizing vs. non-dehumanizing statements. Here, we want to run a mixed model similar to the previous one, regressing reaction time on a binary variable denoting a DH or NDH response, with participant treated as a random effect.

```{r Reaction times}

# Import dataset with 'rt' (reaction time)
experiment_data <- read.csv("~/Downloads/Dehumanization Project/experiment_data.csv")

# Importing the rt associated with trials into 'full_data_long_test' dataset
library(dplyr)
experiment_data_filtered <- experiment_data %>% 
                            filter(trial >= 1 & trial <= 10)
experiment_data_filtered <- experiment_data_filtered %>%
                            rename(Prolific_ID = participant_id)
experiment_data_filtered <- experiment_data_filtered %>% 
                            select(Prolific_ID, trial, rt)
full_data_long_test <- full_data_long_test %>%
                       left_join(experiment_data_filtered, by = c("Prolific_ID", "trial"))


### # Mixed-Effects Linear Regression Model

library(lme4) # Load necessary library
library(lmerTest) # For summaries including p-values

# Converting 'rt' from chr string (in ms) to numeric (in s)
full_data_long_test$rt <- as.numeric(full_data_long_test$rt) 
sum(is.na(full_data_long_test$rt)) # Checking for any NAs
full_data_long_test$rt_seconds <- full_data_long_test$rt / 1000 # Convert ms to s
head(full_data_long_test$rt_seconds)

# The 'choice' column has already been coded as 1 for DH and 0 for NDH (binary variable)
# 'rt' is regressed on binary DH/NDH variable, with Prolific_ID as a random effect
rtmodel <- lmer(rt_seconds ~ choice + (1 | Prolific_ID), 
                            data = full_data_long_test)
summary(rtmodel)


[RUNNING INTO ISSUES WITH RT > NOT THE CORRECT ROWS IT SEEMS LIKE. NOT SURE WHICH RT ROWS FROM THE ORIGINAL DATA "EXPERIMENT_DATA" ARE THE ONES TO KEEP > ASK ALEX] 



```

### Splitting the sample.

### Here, we are splitting the sample between those who scored above and below the median on the "proportion" variable, and comparing their education, gender, and race of the subsamples.

```{r Split sample tests}



```
